# 反正就是鬼扯，哎对对对对对

## 深度学习

### 深度学习的building blocks

之前讲了**感知机**。一个感知机可以完成一个线性二分类。对多种特征分层分类，就得到了神经网络。

神经网络的运作就是一层层的感知机对输入的特征进行抽象，我们可以指望网络从一个高维的空间中抽象出了一种不那么显然（不易用编程显式实现的）的分类方法。

这是有一定生物学意义的。感知机可类比与神经元，神经网络可类比与生物脑中的神经网络。机器神经网络的很多运作模式被证明与生物大脑相似。

### 深度学习能干点啥

神经网络可以放在很多地方。

下面的例子基本都是与深度学习有交集的AI领域

| 。。。。。。|。。。。。。。 | |
|---|---|---|
|分类|**监督学习**|我们有很多的数据，这些数据都有标签（有**先验知识**）。神经网络可以用这些数据训练，训练后的网络多少学得了先验知识，可以对分类未见过的事物。|
|寻找样本特征|**无监督学习**|我们有数据但无足够**先验知识**。因此希望机器自行找到好的特征并据此将之分类。神经网络加持的无监督学习可以很好地在没有先验知识的情况下得到“很合理的分类特征”。本质上人也是一种高性能的无监督学习的模型。|
|最优化|**强化学习**|强化学习是让智能体与环境交互并从中学到策略，以最大化其奖赏。其运行的本质是一种在**马尔科夫过程**上进行的**动态规划**，训练的本质是想办法找马尔科夫过程上奖赏的概率分布。阿尔法go的算法就属于强化学习。对于求解一个最优化序列地的问题，神经网络加持的强化学习可以在麻烦的最优化问题上堆算力来出效果|
|生成|**生成式对抗网络**|包含一个**生成模型**和一个**判别模型**。生成模型根据随机噪音生成东西，判别模型判断东西是否是生成的。生成模型与判别模型构成了一个博弈关系，训练后的生成模型可以用来生成想要的东西，并且以假乱真|

### 训练与调参

深度神经网络中含有很多**感知机**（也叫**神经元**）。因此它有很多参数。这些参数的好坏直接影响模型的效果。可以手动调参，但大多数情况下采用**数据驱动**的方式。

神经网络参数采用**数据驱动**方式的调整被形象地称为“**训练**”。在训练时，定义一个衡量在给定训练数据下模型“好坏”的**损失函数**。对损失函数求关于神经网络参数的梯度并将使用一些**优化算法**将神经网络参数向着**梯度下降**的方向逐步调整，即可以让网络更“好”。

求损失函数关于神经网络梯度的方式叫做**反向传递**，实际上是求梯度的链式法则。由于神经网络可以视作一种计算图，求梯度的链式法则可以很自然地用在这里。

训练这个过程引出了许多新概念。

一些**超参数**：

超参数不是训练过程中模型学得的参数，需要通常需要手动调整或用一些自动调参的办法。

| | |
|-|-|
|learning rate|学习速率。定义了梯度下降的步长|
|epoch|迭代次数。定义了神经网络在训练集中的训练次数|

一些**优化算法**（或者叫“演化策略”）：

这些优化算法负责根据损失函数的梯度以及一些别的玩意求出优化神经网络参数的方向。五花八门，各有千秋（所以细节忘了都）

不过放心吧，我基本只用Adam

| | |
|-|-|
|动量优化|加入动量可以减少梯度下降方向的震荡，加快收敛。注意，这里只是给下一步的计算结果加上了动量，下一步只基于梯度|
|牛顿法(NAG)|动量法的改进。与动量法不同的是在计算下一步方向是考虑了动量。因此下一步的方向不仅与梯度有关，也与当前动量有关|
|Adagrad|自动放缩参数。对于稀疏的特征得到较大幅度的更新，对于稠密的特征采取较小负的更新。用在分类器上可以避免训练时只关注多数而忽略了几个别致的例子|
|RMSProp|在前者的基础上摆脱了对学习速率超参数的依赖|
|Adam|（Adaptive Moment Estimation）自动缩放参数的方法类似动量在阻力下衰减|

模型的**演化策略**不只有梯度下降系列。

一些玄学：

训练程度不当有**欠拟合**、**过拟合**

**欠拟合**需要增加训练次数、增加网络学习能力

**奥卡姆剃刀**的思想告诉我们也要试图防止**过拟合**

| | |
|-|-|
|weight decay|也叫L2正则化。用于防止过拟合。尽量使神经网络中的参数取较小的数|
|learning rate decay|学习速率的下降率。学习速率随时间下降有利于快速（前期）且精确（后期）地找到解|
|Dropout|一种正则化方法，防止过拟合。在每次训练（梯度下降）的过程中随机不激活该神经元。解释一言难尽，姑且玄学|

估计这么多够用了。

## 图论算法

并不试图教会，只打算意会

然后你们会发现我讲的比songyou还形而上，一直搁这搁这:)

### 图是什么

G = <V,E>
图是由节点集合和边集合构成的有序二元组。

其中边集合是有序偶<u,v>的称为**有向图**，是无序偶(u,v)的称为**无向图**

以下就是一个无向图

![avatar](/mdpic/GraphSample.png)

### 图的表示、存储

**邻接矩阵**：

$m_{i,j}$表示是否存在边<i,j>

无向图的邻接矩阵是对称阵

**关联矩阵**：

$m_{i,j}$如此定义：

= 1 当$v_i$是有向边$a_j$的始点
= -1 ~终点
= 0 其他

例：

![avator](/mdpic/GraphMatrixSample.png)

其邻接矩阵为：

![avator](/mdpic/LinJieJuZhen.png)

其关联矩阵为：

![avator](/mdpic/GuanLianJuZhen.png)

**邻接表**：

计算机中用矩阵存储稀疏图是浪费的。没几个边，矩阵的大部分都是0

因此往往采用邻接表存储图，邻接表是数据结构中表的一种

邻接表包含一系列头结点，头结点存储一个链子的地址。链子上是从这个头结点出发的所有边

![avator](/mdpic/LingJieBiao.jfif)

### 图的一些性质

**连通性**：

就是字面意思。

对于无向图，只有联通与不联通之分

对于有向图，联通依据强度不同分为三种

1、**强联通**：从任意顶点出发有抵达任意顶点的通路。非退化的强联通有向图必然包含一个含所有节点的环

2、**单向联通**：从任意顶点a与任意顶点b，若没有a->b的通路就必然有b->a的通路。强连通图是单向联通的

3、**弱联通**：直观来说，化为无向图后是联通的。

由连通性引出的一些算法

判断连通性：

这个很随意，跑一遍多源最短路或者搜索的同时维护并查集都行。值得注意的是单源的算法如单源最短路和Prim只能判断从当前点出发到其他节点的连通性，不等价于整个图的连通性。

连通性也可以用邻接矩阵运算的方式求。关系的矩阵的矩阵乘法对应关系的复合。因此，有n节点图，邻接矩阵的1~n-1次幂之和转布尔值就得到可达性矩阵。顾名思义。

**顶点基**：

在不强联通的图中的一个顶点集合，若从其中某个节点出发可以抵达其余所有节点，而其子集不能，则这是一个顶点基。顶点基不唯一，但其中含有节点数目相同

![avator](/mdpic/DingDianJi.png)

上图中的顶点基有{u,t}、{v,t}、{w,t}

**强连通分量**：

强连通分量全称**强联通分子图**。非常直白，就是一个子图，强联通且最大就叫强连通分量

由强连通分量引出的算法：

求强连通分量：我们并没有学。正反两遍bfs即可

压缩图：因为强联联通分子图内部的节点可以抵达彼此，在求一些与连通性相关的问题时，可以考虑将强连通分量压缩成一个节点，保留强连通分量到外部的所有连接。

压缩后的图中每个节点对应原图顶点基中的一个节点。

**最小生成树**：

注意，最小生成树是无向图上的概念。有向图上的生成树叫**树形图**，求最小树形图的算法是Tarjan算法，此处不讲。

有四种定义树的方法。其中一种：树是无环且联通的图。

一个图的生成树是：包含原图所有顶点，并且其是一棵树的子图。只有连通图具有生成树。不连通的图则拥有一个生成森林。

最小生成树是**边权**和最小的生成树。

最小生成树不一定唯一；最小生成树上的路径一定包含路径端点之间的**瓶颈**，即所有通路中最大边权最小的那条路对应的那条边（瓶颈比这鬼话形象）。这又引出了

求最小生成树的算法：

**Kruskal 算法**：

克鲁斯卡尔算法是一种**贪心算法**。最小生成树，最小边权和且生成树（无环且联通）。

克鲁斯卡尔算法解决“最小”的方法是：首先将所有边按边权升序排序，然后按顺序尝试这些边是否在生成树里。

这个贪心策略的合理性在于如果当前边权最小的边应当被添加到未完工的生成树里，则这个生成树的边权应该也是最小的。

若边在生成树上，则现有未完工的生成树添加这条边后不应该出现环，等价地说，这条边应该联通了之前尚未联通的两个顶点集。

该如何判断两个顶点集之前是否联通？使用**并查集**

在Kru算法的基础上衍生出**Kruskal重构树**等一系列问题，此处不谈。

**Prim 算法**：

较不常用。没啥优势。

Prim算法也是一种贪心算法。在这里补充一下，贪心的反面是**动态规划算法**，这是一种偏向于使用蛮力遍历所有可能状态的算法，在涉及图的问题中很少出现（因为图太复杂，蛮力开销过于巨大）

Prim算法（以及之后的Dijkstra算法）基于一个名为 **“松弛”** 的操作。松弛操作利用了这样一个原理：

在一条a->b的“最短路”的末端加上一条边<b,c>，a->b->c就有可能是a->c的最短路。之所以加引号是因为“最短”的定义在不同需求下可能有所出入。

Prim中“最短”对应的“距离”这么定义：节点到当前正施工的生成树（中任意一个节点）的最短距离

![avator](/mdpic/Prim.gif)

Prim算法：

    初始化所有 节点到生成树距离 ∞
    初始化 总开销累计 为0
    选一个（随机或指定）节点作为树根。距离为0
    由这个节点的邻边初始化 出边终点的距离

    循环，直到：施工中的生成树已包含所有节点
        找到距离施工中生成树最近的节点x（暴力或堆）
        将该节点假如生成树
        由该节点的邻边松弛其他节点y
            松弛：若出边边权+x到生成树的距离<y到生成树的距离，松弛成功。y距离被不等式左侧赋值

            注意！由于x已经被假如生成树，x到生成树的距离 = 0，实际上仅左侧仅有边权。这样写是为方便后面Dij算法的解释

**最短路**：

直白易懂。就是最短路。

有**单源最短路**与**全源最短路**。其中单源最短路算法仅求一个点到其他点的最短路，全源最短路求所有点对之间的最短路。

求最短路的算法：

**Dijkstra算法**：

单源最短路

Djikstra算法也基于松弛。并且这玩意长得跟Prim算法几乎一模一样，除了“距离”的定义，即不等式左侧的式子

这里的“距离”指一个节点到一开始指定节点的距离

![avator](/mdpic/Dij.gif)

可以看到它也在当前“距离”最小的节点进行更新。已求好的树形结构按半径弥散，没有很多不必要的遍历，几乎对所有单元只访问一次，这实际上非常高效。

Dij：

    初始化所有 节点到起点距离 ∞
    初始化 总开销累计 为0
    选一个起点到起点距离为0
    由起点的邻边<a,b>初始化a->b的距离

    循环，直到：施工中的树已包含所有节点
        找到距离起点最近的节点x（暴力或堆）
        将该节点标记已求解
        由该节点的邻边松弛其他节点y
            松弛：若出边边权+x到起点的距离<y到起点的距离，松弛成功。y距离被不等式左侧赋值

注意！ Dij不能应付负权边。负权边会破坏

**Bellman-Ford算法、SPFA**：

Bellman-Ford又是基于松弛的算法。只不过遍历方式与Dijkstra算法不同。

BF不断遍历每条边，看能否松弛某个节点。听起来就很低效。

一种优化基于以下事实：只有上一次被松弛的节点才可能松弛其他节点。可以看看上面Dij算法弥散式生长的那棵树来体会其合理性。

基于这一点，用队列优化BF，松弛后入队，每次仅访问队首元素的边，大大减少了遍历不必要边的开销。

然而最坏情况与原生BF相同。只不过结构简单易于编写，比起Dij算个优势。

**Floyd算法**：

Floyd算法是基于中继的算法。中继实际上就是一种最优子结构：首尾相接的最短路拼接起来仍然是最短路。Floyd就是利用这种最优子结构的DP

转移方程以及遍历方式如下

用三重循环枚举起点、终点、中继点，若

    起点->中继 + 中继->终点 < 起点->终点,更新 起点->终点的距离

**Johnson算法**：

基于DP的Floyd性能堪忧。很多时候

**网络流**：

网络流分为网络与流

网络是一张有向图，流是说每条边容许一个最大流量。这里一条有向边不允许倒流。

可以将其想象为一系列相连的毛细血管。流体从**源**（S）出发，流过网络，抵达**汇**（T）。

![avator](/mdpic/Wangluoliu.webp)

**最大流问题（最小割问题）**：

解决网络流问题

**E-K动能算法**：

**二分图问题**：

**最大匹配（匈牙利算法）**：

### 模拟退火

模拟退火算法是一种求最优解的近似方法。先看看这东西运作时的样子：

求解局部最大值：
![avator](/mdpic/MoNiTuiHuo1.gif)

求解到点集距离和最短点：
![avator](/mdpic/MoNiTuiHuo2.gif)

这个的核心想法就是：

    随机求解，看：是不是当前已知的最优解
        是就接受
        不是，为了不轻易跌入局部最优，也以一定概率接受

那么“退火”体现在哪？

当温度高时随机性更大，随着温度降低，随机性变小。这很自然，分子热运动就有这种特性。

同时，温度的下降速度也与当前温度有关。在现实中的退火温度随时间衰减是一个指数衰减。而模拟退火温度随过程进行的衰减也是指数函数。

这个东西看起来非常合理：一开始温度很高，烫得在可行域内以较为均匀的可能性四处乱蹦。随着温度降低，逐渐冷静下来，倾向于在一个已知的局部最优附近优化。

看看上面梯度下降的优化也有采用类似策略的。

如果说梯度下降算法是在目标函数的表面滑滑梯试图滑到最低点，那么模拟退火就是随便蹦跶找最低点。都是求解析解嘛，大差不差。

引入“退火”后的完整解法：

    def：自行设计一个随机扰动函数X
    def：接受更差解概率（∝过程）衰减的函数
        p = e^(△E/kt)
        其中dE为当前解与上一个解的差值，k为常数，T为当前温度。

    循环，除非：接受更差解的概率过小且长期找不出更优解
        循环，直到：找到更优解或接受更差解
            随机生成一个X，求出Y，与已知最优比较并以p接受更差解

然后再回去看看图，越发觉着这东西挺合理的，还简便。

不放心解的局部性可以多跑几（百、千、万、etc）遍。

## 尾声

**I**：

就没了。不过感觉与其试图讲明白，不如给个单子自己去查。

我想我们不如北航老师吧。但我们听北航老师的课吗？

遍地是优秀的资源，大家需要共享的实际上只有知识点的名称与关联性，或者说，给个目录就行。缺张网而已，不缺鱼

实际上，我只是觉得准备这讲稿麻烦，并且我觉得听了较看一遍目录也没大用。理由：比起给目录，反正想会还得花几乎等量的精力自学。（不是完全没用，奈何markdown里打不出狗头保命）

以这样的条目分享或许会更有效率。

    <知识点名称>-[知识点类别、用途]-(与其他知识点的联系)-{可选：知识点梗概}

**II**：

可以用github等平台管理我们的资源。代码、论文、资料等全部分类置于一个队伍公共的仓库中管理。可以用issue工具进行非正式的讨论（比微信群强）。现在咱们的资料结构是真的乱。

这个仓库属于既网之后的渔获共享了
